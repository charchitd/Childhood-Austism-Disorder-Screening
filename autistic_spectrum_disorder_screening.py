# -*- coding: utf-8 -*-
"""Autistic_SPectrum_Disorder_Screening.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZzqKbfgQfBCJfXLLlOq9eO8zADlgqXgK
"""

import sys
import pandas as pd
import sklearn
import keras

print ('Python: {}'.format(sys.version))
print ('Pandas: {}'.format(pd.__version__))
print ('Sklearn: {}'.format(sklearn.__version__))
print ('Keras: {}'.format(keras.__version__))



from google.colab import files
uploaded = files.upload()

import io



data = pd.read_csv(io.BytesIO(uploaded['Autism-Child-Data.csv']))

data.head()

data = data.drop(['id'],axis=1)
print('Shape of DataFrame: {}'.format(data.shape))
print(data.loc[0])

# print out multiple patients at the same time
data.loc[:10]

data.describe()

# drop unwanted columns
data = data.drop(['result', 'age_desc'], axis=1)

data.loc[:10]

# create X and Y datasets for training
x = data.drop(['Class/ASD'], 1)
y = data['Class/ASD']

x.loc[:10]

# convert the data to categorical values - one-hot-encoded vectors
X = pd.get_dummies(x)

X.loc[:10]

# print the new categorical column labels
X.columns.values

# print an example patient from the categorical data
X.loc[1]

# convert the class data to categorical values - one-shot-encoded vectors
Y = pd.get_dummies(y)

Y.loc[:10]

Y.columns.values

print('Shape of DataFrame: {}'.format(X.shape))

#  Split the Dataset into Training and Testing Datasets

from sklearn import model_selection
# split the X and Y data into training and testing datasets
X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size = 0.2)

print (X_train.shape)
print (X_test.shape)
print (Y_train.shape)
print (Y_test.shape)

#  Building the Network with Keras

from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.optimizers import Adam

# define a function to build the keras model
def create_model():
    # create model
    model = Sequential()
    model.add(Dense(8, input_dim=96, kernel_initializer='normal', activation='relu'))
    model.add(Dense(4, kernel_initializer='normal', activation='relu'))
    model.add(Dense(2, activation='sigmoid'))
    model.add(Dropout(0.25))
    
    # compile model
    adam = Adam(lr=0.001)
    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])
    return model

model = create_model()

print(model.summary())

# Train the model

# fit the model to the training data
model.fit(X_train, Y_train, epochs=50, batch_size=10, verbose = 1)

# Testing the model data

# generate classification report using predictions for categorical model
from sklearn.metrics import classification_report, accuracy_score

predictions = model.predict_classes(X_test)
predictions

print('Results for Categorical Model')
print(accuracy_score(Y_test[['YES']], predictions))
print(classification_report(Y_test[['YES']], predictions))

# Now that our model has been trained, we need to test its performance on the testing dataset. 
# The model has never seen this information before; as a result, the testing dataset allows us to determine whether or not the model will be able to generalize to information that wasn't used during its training phase.
# We will use some of the metrics provided by scikit-learn for this purpose!